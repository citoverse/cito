% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cnn.R
\name{cnn}
\alias{cnn}
\title{Train a Convolutional Neural Network (CNN)}
\usage{
cnn(
  X,
  Y = NULL,
  architecture,
  loss = c("mse", "mae", "cross-entropy", "bernoulli", "gaussian", "binomial", "poisson",
    "mvp", "nbinom", "multinomial", "clogit", "softmax"),
  custom_parameters = NULL,
  optimizer = c("sgd", "adam", "adadelta", "adagrad", "rmsprop", "rprop", "ignite_adam"),
  lr = 0.01,
  lr_scheduler = NULL,
  alpha = 0.5,
  lambda = 0,
  validation = 0,
  batchsize = NULL,
  shuffle = TRUE,
  data_augmentation = NULL,
  epochs = 100,
  early_stopping = Inf,
  burnin = Inf,
  baseloss = NULL,
  device = c("cpu", "cuda", "mps"),
  plot = TRUE,
  verbose = TRUE
)
}
\arguments{
\item{X}{An array of input data with a minimum of 3 and a maximum of 5 dimensions. The first dimension represents the samples, the second dimension represents the channels, and the third to fifth dimensions represent the input dimensions. As an alternative, you can provide the relative or absolute path to the folder containing the images. In this case, the images will be normalized by dividing them by 255.0.}

\item{Y}{The target data. The allowed formats of the target data differ between loss functions. See \code{\link{dnn}} for more information.}

\item{architecture}{An object of class 'citoarchitecture'. See \code{\link{create_architecture}} for more information.}

\item{loss}{The loss function to be used. Options include "mse", "mae", "cross-entropy", "bernoulli", "gaussian", "binomial", "poisson", "nbinom", "mvp", "multinomial", and "clogit". You can also specify your own loss function. See Details for more information. Default is "mse".}

\item{custom_parameters}{Parameters for the custom loss function. See the vignette for an example. Default is NULL.}

\item{optimizer}{The optimizer to be used. Options include "sgd", "adam", "adadelta", "adagrad", "rmsprop", "rprop", and "ignite_adam". See \code{\link{config_optimizer}} for further adjustments to the optimizer. Default is "sgd".}

\item{lr}{Learning rate for the optimizer. Default is 0.01.}

\item{lr_scheduler}{Learning rate scheduler. See \code{\link{config_lr_scheduler}} for creating a learning rate scheduler. Default is NULL.}

\item{alpha}{Alpha value for L1/L2 regularization. Default is 0.5.}

\item{lambda}{Lambda value for L1/L2 regularization. Default is 0.0.}

\item{validation}{Proportion of the data to be used for validation. Alternatively, a vector containing the indices of the validation samples can be provided. Default is 0.0.}

\item{batchsize}{Batch size for training. If NULL, batchsize is 10\% of the training data. Default is NULL.}

\item{shuffle}{Whether to shuffle the data before each epoch. Default is TRUE.}

\item{data_augmentation}{A list of functions used for data augmentation. Elements must be either functions or strings corresponding to inbuilt data augmentation functions. See details for more information.}

\item{epochs}{Number of epochs to train the model. Default is 100.}

\item{early_stopping}{Number of epochs with no improvement after which training will be stopped. Default is Inf.}

\item{burnin}{Number of epochs after which the training stops if the loss is still above the baseloss. Default is Inf.}

\item{baseloss}{Baseloss used for burnin and plot. If NULL, the baseloss corresponds to intercept only models. Default is NULL.}

\item{device}{Device to be used for training. Options are "cpu", "cuda", and "mps". Default is "cpu".}

\item{plot}{Whether to plot the training progress. Default is TRUE.}

\item{verbose}{Whether to print detailed training progress. Default is TRUE.}
}
\value{
An S3 object of class \code{"citocnn"} is returned. It is a list containing everything there is to know about the model and its training process.
The list consists of the following attributes:
\item{net}{An object of class "nn_module". Originates from the torch package and represents the core object of this workflow.}
\item{call}{The original function call.}
\item{loss}{An object of class "nn_module". Contains all relevant information for the loss function, e.g. parameters and a function (format_Y) that transforms target data.}
\item{data}{A list. Contains the data used for the training of the model.}
\item{model_properties}{A list of properties, that define the architecture of the model.}
\item{training_properties}{A list of all training hyperparameters used the last time the model was trained.}
\item{losses}{A data.frame containing training and validation losses of each epoch.}
\item{best_epoch_net_state_dict}{Serialized state dict of net from the best training epoch.}
\item{best_epoch_loss_state_dict}{Serialized state dict of loss from the best training epoch.}
\item{last_epoch_net_state_dict}{Serialized state dict of net from the last training epoch.}
\item{last_epoch_net_state_dict}{Serialized state dict of loss from the last training epoch.}
\item{use_model_epoch}{String, either "best" or "last". Determines whether the parameters (e.g. weights, biases) from the best or the last training epoch are used (e.g. for prediction).}
\item{loaded_model_epoch}{String, shows from which training epoch the parameters are currently loaded in \code{net} and \code{loss}.}
}
\description{
This function trains a Convolutional Neural Network (CNN) on the provided input data \code{X} and the target data \code{Y} using the specified architecture, loss function, and optimizer.
}
\section{Details:}{
Also check \code{\link{dnn}} for details to common arguments.
}

\section{Convolutional Neural Networks:}{
Convolutional Neural Networks (CNNs) are a specialized type of neural network designed for processing structured data, such as images.
The key components of a CNN are convolutional layers, pooling layers and fully-connected (linear) layers:
\itemize{
\item \strong{Convolutional layers} are the core building blocks of CNNs. They consist of filters (also called kernels), which are small, learnable matrices. These filters slide over the input data to perform element-wise multiplication, producing feature maps that capture local patterns and features. Multiple filters are used to detect different features in parallel. They help the network learn hierarchical representations of the input data by capturing low-level features (edges, textures) and gradually combining them (in subsequent convolutional layers) to form higher-level features.
\item \strong{Pooling layers} reduce the size of the feature maps created by convolutional layers, while retaining important information. A common type is max pooling, which keeps the highest value in a region, simplifying the data while preserving essential features.
\item \strong{Fully-connected (linear) layers} connect every neuron in one layer to every neuron in the next layer. These layers are found at the end of the network and are responsible for combining high-level features to make final predictions.
}

The architecture of the CNN that will be created and trained by this function is defined by an object of class 'citoarchitecture'. See \code{\link{create_architecture}} for detailed information on how to define and customize your CNN architecture.
}

\section{Data Augmentation}{
Data augmentation is a technique used to improve the generalization of convolutional neural networks (CNNs) by increasing the diversity of the training data through random transformations. This function supports data augmentation through the \code{data_augmentation} argument, which accepts a list containing either user-defined functions or the names of cito's built-in data augmentation functions.
Each user-defined function must take a \code{torch_tensor} as input and return a \code{torch_tensor} with the same shape. The input tensor must have 3 to 5 dimensions with the following structure:
\itemize{
\item Dimension 1: singleton batch dimension (i.e., size 1),
\item Dimension 2: channel dimension,
\item Dimensions 3 to 5: spatial dimensions (e.g., X, Y, Z).
}
During training, the data loader re-loads each sample at every epoch, applying all provided augmentation functions sequentially each time the sample is accessed. This allows transformations to vary across epochs if the functions include randomness (e.g., randomly flipping a spatial axis) helping the model learn invariance to such changes.
In addition to custom functions, the list can contain the names (as strings) of the following built-in augmentation methods:
\itemize{
\item \code{"rotate90"}:
\itemize{
\item For 2D convolutions: randomly applies one of the 4 possible 90° rotations. The X and Y dimensions have to be equal.
\item For 3D convolutions:
\itemize{
\item If X, Y, and Z dimensions are equal: randomly applies one of the 24 possible 90° rotations.
\item If only two spatial dimensions are equal: randomly applies one of the 4 possible 90° rotations in the plane of the two spatial dimensions.
}
\item Not available for 1D convolutions.
}
\item \code{"flip"}: Randomly flips each spatial dimension independently with 50\% probability.
\item \code{"noise"}: Adds a small amount of normally distributed noise to the tensor.
}
}

\section{Training and convergence of neural networks}{


Ensuring convergence can be tricky when training neural networks. Their training is sensitive to a combination of the learning rate (how much the weights are updated in each optimization step), the batch size (a random subset of the data is used in each optimization step), and the number of epochs (number of optimization steps). Typically, the learning rate should be decreased with the size of the neural networks (depth of the network and width of the hidden layers). We provide a baseline loss (intercept only model) that can give hints about an appropriate learning rate:

\figure{learningrates.jpg}{Learning rates}

If the training loss of the model doesn't fall below the baseline loss, the learning rate is either too high or too low. If this happens, try higher and lower learning rates.

A common strategy is to try (manually) a few different learning rates to see if the learning rate is on the right scale.

See the troubleshooting vignette (\code{vignette("B-Training_neural_networks")}) for more help on training and debugging neural networks.
}

\examples{
\donttest{
if(torch::torch_is_installed()){
library(cito)

# Example workflow in cito

device <- ifelse(torch::cuda_is_available(), "cuda", "cpu")

## Data
### We generate our own data:
### 320 images (3x50x50) of either rectangles or ellipsoids
shapes <- cito:::simulate_shapes(n=320, size=50, channels=3)
X <- shapes$data
Y <- shapes$labels

## Architecture
### Declare the architecture of the CNN
### Note that the output layer is added automatically by cnn()
architecture <- create_architecture(conv(5), maxPool(), conv(5), maxPool(), linear(10))

## Build and train network
### softmax is used for classification
cnn.fit <- cnn(X, Y, architecture, loss = "cross-entropy", epochs = 50, validation = 0.1, lr = 0.05, device=device)

## The training loss is below the baseline loss but at the end of the
## training the loss was still decreasing, so continue training for another 50
## epochs
cnn.fit <- continue_training(cnn.fit, epochs = 50)

# Structure of Neural Network
print(cnn.fit)

# Plot Neural Network
plot(cnn.fit)

## Convergence can be tested via the analyze_training function
analyze_training(cnn.fit)

## Transfer learning
### With the transfer() function we can use predefined architectures with pretrained weights
transfer_architecture <- create_architecture(transfer("resnet18"))
resnet <- cnn(X, Y, transfer_architecture, loss = "cross-entropy",
              epochs = 10, validation = 0.1, lr = 0.05, device=device)
print(resnet)
plot(resnet)
}
}
}
\seealso{
\code{\link{predict.citocnn}}, \code{\link{print.citocnn}}, \code{\link{plot.citocnn}}, \code{\link{summary.citocnn}}, \code{\link{coef.citocnn}}, \code{\link{continue_training}}, \code{\link{analyze_training}}
}
\author{
Armin Schenk
}
