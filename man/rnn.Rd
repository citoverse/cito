% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/rnn.R
\name{rnn}
\alias{rnn}
\title{RNN}
\usage{
rnn(
  formula,
  data = NULL,
  loss = c("mae", "mse", "softmax", "cross-entropy", "gaussian", "binomial", "poisson"),
  type = c("gru", "lstm"),
  hidden_size = 64,
  num_layers = 1,
  lag = 3,
  validation = 0,
  bias = TRUE,
  lambda = 0,
  alpha = 0.5,
  dropout = 0,
  optimizer = c("adam", "adadelta", "adagrad", "rmsprop", "rprop", "sgd", "lbfgs"),
  lr = 0.01,
  batchsize = 32L,
  shuffle = FALSE,
  epochs = 32,
  plot = TRUE,
  verbose = TRUE,
  lr_scheduler = NULL,
  device = c("cpu", "cuda"),
  early_stopping = FALSE
)
}
\arguments{
\item{formula}{an object of class "\code{\link[stats]{formula}}": a description of the model that should be fitted}

\item{data}{matrix or data.frame}

\item{type}{Defines which type of RNN is fitted, either gated recurrent units (gru) or long short-term memory (lstm)}

\item{hidden_size}{Number of features in hidden state of lstm or gru}

\item{num_layers}{Number of rnn layers, if >1 then a stacked gru/lstm is created}

\item{lag}{How many iterations before \eqn{Y_i} are used for prediction, \eqn{X_{i-1,i-(lag+1)}}}

\item{validation}{percentage of data set that should be taken as validation set (the last iterations are chosen)}

\item{bias}{whether use biases in the layers, can be of length one, or a vector (number of hidden layers + 1 (last layer)) of logicals for each layer.}

\item{lambda}{strength of regularization: lambda penalty, \eqn{\lambda * (L1 + L2)} (see alpha)}

\item{alpha}{add L1/L2 regularization to training  \eqn{(1 - \alpha) * |weights| + \alpha ||weights||^2} will get added for each layer. Can be single integer between 0 and 1 or vector of alpha values if layers should be regularized differently.}

\item{dropout}{dropout rate, probability of a node getting left out during training (see \code{\link[torch]{nn_dropout}})}

\item{optimizer}{which optimizer used for training the network, for more adjustments to optimizer see \code{\link{config_optimizer}}}

\item{lr}{learning rate given to optimizer}

\item{batchsize}{number of samples that are used to calculate one learning rate step}

\item{shuffle}{if TRUE, data in each batch gets reshuffled every epoch}

\item{epochs}{epochs the training goes on for}

\item{plot}{plot training loss}

\item{verbose}{print training and validation loss of epochs}

\item{lr_scheduler}{learning rate scheduler created with \code{\link{config_lr_scheduler}}}

\item{device}{device on which network should be trained on.}

\item{early_stopping}{if set to integer, training will stop if validation loss worsened between current defined past epoch.}
}
\value{
an S3 object of class \code{"citornn"} is returned. It is a list containing everything there is to know about the model and its training process.
The list consists of the following attributes:
\item{net}{An object of class "nn_sequential" "nn_module", originates from the torch package and represents the core object of this workflow.}
\item{call}{The original function call}
\item{loss}{A list which contains relevant information for the target variable and the used loss function}
\item{losses}{A data.frame containing training and validation losses of each epoch}
\item{data}{Contains data used for training the model}
\item{weigths}{List of weights for each training epoch}
}
\description{
fits a recurrent neural network. rnn() supports the formula syntax and allows to customize the shape of it to a maximal degree.
It is possible to fit gated recurrent unit RNNs (GRU) or multi-layer long short-term memory RNNs (LSTM). To learn more about Deep Learning, see \href{https://www.nature.com/articles/nature14539}{here}
}
\details{
value
}
\examples{
\dontrun{
library(cito)

set.seed(222)
validation_set<- sample(c(1:nrow(datasets::iris)),25)

# Build and train  Network
nn.fit<- dnn(Sepal.Length~., data = datasets::iris[-validation_set,])

# Sturcture of Neural Network
print(nn.fit)

# Use model on validation set
predictions <- predict(nn.fit, iris[validation_set,])

# Scatterplot
plot(iris[validation_set,]$Sepal.Length,predictions)
# MAE
mean(abs(predictions-iris[validation_set,]$Sepal.Length))

# Get variable importances
summary(nn.fit)

# Partial dependencies
PDP(nn.fit, variable = "Petal.Length")

# Accumulated local effect plots
ALE(nn.fit, variable = "Petal.Length")


}
}
\seealso{
\code{\link{predict.citornn}}, \code{\link{plot.citornn}},  \code{\link{coef.citornn}},\code{\link{print.citornn}}, \code{\link{summary.citornn}}, \code{\link{continue_training}}, \code{\link{analyze_training}}
}
